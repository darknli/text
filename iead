目前的confidence其实并不是bbox的概率,而是anchor的概率,这显然是有问题的,因此应该先得到bbox,然后用bbox去feature map上裁剪,在用裁剪后的feature map得到confidence,这样才是真正bbox对应的confidence,
目前基于深度学习的跟踪算法基本都是Siamese net的思想,用模板图和目标图
        卷积后的特征做一个相关滤波,找到最大相应的那个区域就是跟踪目标,其实检
        测网络也可以认为是图中的每个区域去和训练集的平均特
                
                征做比较,当相似度大于一定程度则认为是
                检测目标,因此可以把这个潜在的平均特征替换成我们显式的模板,然后
是想一个问题：如果在A数据集训练
一个目标检测任务,杯子是否箱子挡住以及杯子的位置,在B数据集只训练勺子是否被箱子挡住的分类问
题,模型是否具有回归勺子位置的泛化能力？假设是人类大脑,在学习了A数据集的规则之后,告诉你B数据集中的某张图杯子被箱子挡住,可以很顺利地知道杯子在箱子附近,
是否可以通过已有标签生成新的任务的标签,来加快收敛,甚至提高泛化能力？
把它作为模型参数作为卷积核存储在模型里,预测的时候将图片和这些卷积核做卷积求得最大相应,如果相应大于某个阈值则认为检测到了目标,
一、分割实验
    以下实验均按顺序执行,

1.1 用部分数据集（为了加快速度）做了消融试验,decoder部分的bn层可以不加,效果不提升也不降；注意力部分的残差结构加上,验证集的dc可以提高0.4个点（86.3->86.7）,而且收敛速度变快,结论：保留残差,去掉bn,

1.2 在以头为中心的基础上添加了以胸部为中心的裁剪方式,更加保证上半身的检测效果,验证集dc是86.9（上周最好的结果是85.5）；实际体验上胳膊的漏检情况也少了更多,
    结论：该裁剪方式有效,

1.3 只有胸部以上的人像数据集mattingHumanhalf,标注质量高于之前的几个数据集平均水平,尤其是边界效果（以此观之,该数据集非常简单,我们模型可以在这个数据集上dc达到97）,用我们的方法训练该数据集,模型边缘效果只有细微提升,
    这给了我们一些启示：目前的模型已经在现有的数据集上达到一定瓶颈,后面优化的方向不再是如何更好地拟合当前数据集,而是在给定标注质量不高的情况下,更好地学到真正有用的信息,

1.4 将人像边缘附近的训练权重设为0,具体做法是对原mask分别腐蚀&膨胀,将那些区域的权重设为0,训练的前期loss下降很慢,但dc却涨的很快,该模型在验证集下dc是87.1
在简单场景下表现一致,复杂场景下与上一个最好的模型,各有胜负,差不多六四开,要稍好一些,边缘效果基本没有改善,
    结论：该训练方法有效,但依然没有解决目前模型对于边界效果差的缺陷,

1.5 开始实验使用预训练模型,目前来看初期收敛很快但在100+轮的时候与之前的训练曲线差距越来越小,

二、分割demo
2.1 对分割网络给出的mask再做一些调整使其效果更佳：对比了几种工程（canny、gabor等）和matting方法（贝叶斯、closed form、pymatting等）,densecrf可以更好地提升效果,包括边缘效果和手指检出效果,但需要背景与人像的颜色区分度较大,
2.2 已写完c++版的图片&视频的分割代码（还差个densecrf,这个有点难搞,补完要花一些时间）

三、其他
3.1 找到了一个自动抠图网站https://www.remove.bg/zh,可为后续需要,集中补充特定类型数据集,
3.2 测试了性能,下面分别是不同宽度因子的全网络和encoder部分网络（shufflenetv2）,可以看出,decoder部分非常耗时,原因是encoder的卷积步长是2,而decoder的卷积步长是1,下图的encoder之和shufflenetv2差最后的全连接和一个卷积层,
模型
耗时
mutil-mask-unet 0.5
106ms
muitl-mask-unet 0.3
92ms
单batckbone, encoder 0.5
31ms
单backbone, encoder 0.3
20ms
四、下周计划
4.1 做一个关键点的标注工具,至少有以下功能
	*
如果给定112×112的人脸,可以将点位置的精度到小数点后一位,可将图片扩大十倍以精标注
	*
支持鼠标或键盘
	*
可以标记是否可见,是否遮挡


4.2 开始训练属性（年龄性别等）网络
一、鉴黄
1.1 解决视频检测时,内存疯长的问题：添加设置 LRU_CACHE_CAPACITY=N（N代表缓存容量）,除了终端命令,也可在python内部的os.environ设置,
1.2 新版本上线后,人审量与v0.1版本持平；目前没有用户的漏检反馈

二、人脸检测sdk
2.1 解决lnet调用析构函数的时候,偶现崩溃的问题(有指针重复释放)

三、人体分割
3.1 完成了shufflenetv2为backbone的unet代码,并发现了一个问题：
	* 
    在batchsize=1的gpu环境下,shuffleNet推理速度比unet全模型要慢（mobilenet等小模型都有这个问题）
	* 
    问题原因：当batchsize=1时,计算开销会变得非常小,导致其他开销占总体耗时的比例就增大,加上shufflenet等小模型的层数通常要多于unet,开销要大得多,因此总耗时会比unet大（哪怕增加一层relu,都会让整体的耗时有可见的增长）,


3.2 加入随机裁剪的数据增强代码,以便可以将训练的batch设成多个,试了batch=32到batch=256,模型的效果差距不大,32batch的模型到最后反而效果更稍好一些,
3.3 尝试了warmup优化模型,调了几组参数效果都不如adam理想,
3.4 训练了ATR+LIP数据集,模型在EG1800测试集上的最高mIOU是82.36,主要的问题有以下三点：
    1）脖子黑了一块,解决方案：从输入数据开始追查问题
    2）闭环内如果不是非人体,也会无分为人体,解决方案：加大边界的权重,
    3）如果画面没有人的话也会有一些地方被分为人体区域,解决方案：加入丰富的纯背景数据,
    检查数据集之后,发现：
    1）LIP有以下问题：
	* 
脖子及胸部位认为是背景
	* 
多人同屏时,如果一个人的一个部分连接到了另一个人的主要区域,直接被砍掉


    2）ATR有以下问题：
	* 
大部分是女性
	* 
姿态不丰富
	* 
图内有少量噪声


3.5 将数据集改为CIHP（比LIP要少一些,有3.3万张）,模型在EG1800测试集上的最高mIOU是92.56,目前最大的问题有两个：
    1）胳膊和手检不同姿态测效果不好
    2）图内没有人时,依然会把背景检成人体

四、下周计划
4.1 将shufflenetv2,宽度是0.5为backbone,做优化
4.2 增加更多比例的纯背景样本,以解决图内没有人时,将背景检成人体的问题
4.3 依次引入剩下已有的数据集解决手和胳膊不同姿态导致效果差的问题
4.4 依次试验mixup、gcl、self attention distillation、pointrend head、focal loss、ohem等技术优化模型在边缘等情况的效果